{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Problem 1 [50 points] Calculus of Variations in Probability",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Probability density function (PDF) of an $n$-dimensional continuous random vector $X$ is a deterministic function $u(x)$, where $x\\in\\Omega\\subseteq\\mathbb{R}^{n}$, such that $u(x)\\geq 0$, and $\\int_{\\Omega}u(x){\\mathrm{d}}x = 1$. The expected value of any nonlinear function $\\phi(x)$, denoted here as $\\mathbb{E}\\left[\\phi(x)\\right]$, is defined as the integral\n$$\\mathbb{E}\\left[\\phi(x)\\right] := \\int_{\\Omega}\\phi(x)u(x){\\mathrm{d}}x.$$\nA well-known example of the expectation integral of above type is the **entropy** $\\mathbb{E}\\left[-\\log u(x)\\right] = -\\int_{\\Omega}u(x)\\log u(x){\\mathrm{d}}x$, i.e., $\\phi(\\cdot)\\equiv -\\log u(\\cdot)$. In this exercise, we will assume that $\\Omega$ is a compact set, and denote its (Lebesgue) volume as ${\\mathrm{vol}}(\\Omega) := \\int_{\\Omega}{\\mathrm{d}}x$.\n\nFor all problems in this exercise, the existence-uniqueness of minimizers can be guaranteed from convexity arguments. So you don't need to worry about those. Instead, the purpose of this exercise is to apply the Euler-Lagrange equation to actually compute the solutions.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## (a) [10 + (4 + 1) = 15 points] Maximizing entropy vs. minimizing expectation\n\nGiven a non-negative function $f(x)$, and a scalar $\\beta\\geq 0$, we want to determine the PDF $u(x)$ that minimizes\n$$I(u) = \\mathbb{E}\\left[\\log u + \\beta\\, f\\right] = \\int_{\\Omega}\\big\\{u(x)\\log u(x) + \\beta\\, f(x)u(x)\\big\\}{\\mathrm{d}}x.$$\nBy changing the value of $\\beta$, we can trade-off the relative importance of maximizing entropy and minimizing the expected value of $f$. In optimization literature, $\\beta$ is called a \"regularizing parameter\". \n\n(a.1) Using Euler-Lagrange equation, determine the PDF $u^{*}$ that minimizes $I(u)$. \n\n(a.2) By specializing your answer in (a.1) for $\\beta=0$, determine $u_{0}^{*}$. **Explain in one sentence** why this result matches your intuition.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## (b) [15 + 20 = 35 points] Expectation constrained entropy maximization\n\nSuppose now we would like to minimize $I(u) = \\mathbb{E}\\left[\\log u\\right]$ subject to the constraints $\\mathbb{E}\\left[g_{i}(x)\\right] = a_{i}$, $i=1,...,m$, where the vectors $g(x) = (g_{1}(x), ..., g_{m}(x))^{\\top}$ and $a = (a_1, ..., a_m)^{\\top}$ are given.\n\n(b.1) Prove that the minimizing PDF $u^{*}$ must be of exponential family, i.e., \n\n$$u^{*}(x) = \\displaystyle\\frac{\\exp\\left(-\\langle\\lambda,g(x)\\rangle\\right)}{\\int_{\\Omega}\\exp\\left(-\\langle\\lambda,g(x)\\rangle\\right){\\mathrm{d}}x}$$\n\nfor some $m\\times 1$ vector $\\lambda$ that is independent of $x$.\n\n(b.2) Specializing your answer in part (b.1) for an 1D interval $\\Omega = [a,b]$, **show that** the maximum entropy PDF that has a given mean $\\mathbb{E}\\left[x\\right] = \\overline{x}\\in(a,b)$, must be of the form\n\n$$u^{*}(x) = \\dfrac{e^{-\\lambda x}}{Z}$$\n\nwhere the constants $Z$ and $\\lambda$ satisfy **two equations that you need to derive**. You don't need to solve these equations.",
      "metadata": {}
    }
  ]
}