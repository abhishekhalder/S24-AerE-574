{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Problem 1 [50 points] Calculus of Variations in Probability",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Probability density function (PDF) of an $n$-dimensional continuous random vector $X$ is a deterministic function $u(x)$, where $x\\in\\Omega\\subseteq\\mathbb{R}^{n}$, such that $u(x)\\geq 0$, and $\\int_{\\Omega}u(x){\\mathrm{d}}x = 1$. The expected value of any nonlinear function $\\phi(x)$, denoted here as $\\mathbb{E}\\left[\\phi(x)\\right]$, is defined as the integral\n$$\\mathbb{E}\\left[\\phi(x)\\right] := \\int_{\\Omega}\\phi(x)u(x){\\mathrm{d}}x.$$\nA well-known example of the expectation integral of above type is the **entropy** $\\mathbb{E}\\left[-\\log u(x)\\right] = -\\int_{\\Omega}u(x)\\log u(x){\\mathrm{d}}x$, i.e., $\\phi(\\cdot)\\equiv -\\log u(\\cdot)$. In this exercise, we will assume that $\\Omega$ is a compact set, and denote its (Lebesgue) volume as ${\\mathrm{vol}}(\\Omega) := \\int_{\\Omega}{\\mathrm{d}}x$.\n\nFor all problems in this exercise, the existence-uniqueness of minimizers can be guaranteed from convexity arguments. So you don't need to worry about those. Instead, the purpose of this exercise is to apply the Euler-Lagrange equation to actually compute the solutions.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## (a) [10 + (4 + 1) = 15 points] Maximizing entropy vs. minimizing expectation\n\nGiven a non-negative function $f(x)$, and a scalar $\\beta\\geq 0$, we want to determine the PDF $u(x)$ that minimizes\n$$I(u) = \\mathbb{E}\\left[\\log u + \\beta\\, f\\right] = \\int_{\\Omega}\\big\\{u(x)\\log u(x) + \\beta\\, f(x)u(x)\\big\\}{\\mathrm{d}}x.$$\nBy changing the value of $\\beta$, we can trade-off the relative importance of maximizing entropy and minimizing the expected value of $f$. In optimization literature, $\\beta$ is called a \"regularizing parameter\". \n\n(a.1) Using Euler-Lagrange equation, determine the PDF $u^{*}$ that minimizes $I(u)$. \n\n(a.2) By specializing your answer in (a.1) for $\\beta=0$, determine $u_{0}^{*}$. **Explain in one sentence** why this result matches your intuition.\n\n## Solution for part (a):\n\n(a.1) Let $\\lambda$ be the Lagrange multiplier associated with the normalization constraint $\\int_{\\Omega} u = 1$, which is an integral equality constraint. Then the augmented Lagrangian $L(x,u,\\nabla u) + \\lambda M(x,u,\\nabla u) = u\\log u + \\beta f(x) u + \\lambda\\:u$, results in the EL equation:\n$$\\frac{\\partial}{\\partial u}\\left(L+\\lambda M\\right) - \\nabla\\cdot\\frac{\\partial}{\\partial \\nabla u}\\left(L+\\lambda M\\right) = 1 + \\log u + \\beta f(x) + \\lambda = 0,$$\nwhich gives optimizer $u^{*} = \\exp\\left(-\\left(1 + \\lambda\\right) - \\beta f(x)\\right)$. To determine $\\lambda$, we employ the feasibility condition $\\int_{\\Omega} u = 1$, to get\n$$\\exp\\left(-\\left(1 + \\lambda\\right)\\right) = \\frac{1}{\\int_{\\Omega}\\exp\\left(- \\beta f(x)\\right){\\rm{d}}x}.$$\nSubstituing the above back in the RHS of $u^{*}$, we obtain\n$$u^{*} = \\frac{\\exp\\left(- \\beta f(x)\\right)}{\\int_{\\Omega}\\exp\\left(- \\beta f(x)\\right){\\rm{d}}x},$$\nwhich is the Gibbs PDF. Notice that we did not need to explicitly enforce the pointwise inequality constraint $u(x) \\geq 0$ for all $x\\in\\Omega$, since the exponential function returns non-negative values.\n\n(a.2) By substituting $\\beta=0$ in the expression for $u^{*}$ obtained in part (a.1), we get that\n$$u^{*}(x)\\bigg\\vert_{\\beta=0} = \\frac{1}{\\text{vol}\\left(\\Omega\\right)},$$\nwhich is a uniform PDF supported over $\\Omega$. The interpretation is that the uniform PDF \nmaximizes entropy over a compact support, which intuitively makes sense since in the absence of additional information, maximum entropy occurs when all values in $\\Omega$ are equally likely.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## (b) [15 + 20 = 35 points] Expectation constrained entropy maximization\n\nSuppose now we would like to minimize $I(u) = \\mathbb{E}\\left[\\log u\\right]$ subject to the constraints $\\mathbb{E}\\left[g_{i}(x)\\right] = a_{i}$, $i=1,...,m$, where the vectors $g(x) = (g_{1}(x), ..., g_{m}(x))^{\\top}$ and $a = (a_1, ..., a_m)^{\\top}$ are given.\n\n(b.1) Prove that the minimizing PDF $u^{*}$ must be of exponential family, i.e., \n\n$$u^{*}(x) = \\displaystyle\\frac{\\exp\\left(-\\langle\\lambda,g(x)\\rangle\\right)}{\\int_{\\Omega}\\exp\\left(-\\langle\\lambda,g(x)\\rangle\\right){\\mathrm{d}}x}$$\n\nfor some $m\\times 1$ vector $\\lambda$ that is independent of $x$.\n\n(b.2) Specializing your answer in part (b.1) for an 1D interval $\\Omega = [a,b]$, **show that** the maximum entropy PDF that has a given mean $\\mathbb{E}\\left[x\\right] = \\overline{x}\\in(a,b)$, must be of the form\n\n$$u^{*}(x) = \\dfrac{e^{-\\lambda x}}{Z}$$\n\nwhere the constants $Z$ and $\\lambda$ satisfy **two equations that you need to derive**. You don't need to solve these equations.\n\n## Solution for part (b):\n\n(b.1) For $i=1,...,m$, let $\\lambda_{i}$ be the Lagrange multiplier associated with the $i$-th integral equality constraint $\\mathbb{E}\\left[g_{i}(x)\\right]=a_{i}$. Furthermore, let $\\mu$ be the Lagrange multiplier associated with the normalization constraint $\\int_{\\Omega}u=1$. Letting $\\lambda := \\left(\\lambda_{1}, ..., \\lambda_{m}\\right)^{\\top}$, $\\nu:=(\\mu, \\lambda)^{\\top}\\in\\mathbb{R}^{m+1}$, the augmented Lagrangian $L+\\langle\\nu,M\\rangle= u\\log u + \\mu u + \\langle \\lambda, g(x)u \\rangle$. Therefore, the EL equation gives\n$$\\frac{\\partial}{\\partial u}\\left(L+\\nu M\\right) - \\nabla\\cdot\\frac{\\partial}{\\partial \\nabla u}\\left(L+\\nu M\\right) = 1 + \\log u + \\mu + \\langle\\lambda,g(x)\\rangle = 0,$$\nwhich yields optimizer $u^{*} = \\exp\\left(-1 - \\mu - \\langle\\lambda,g(x)\\rangle\\right)$. Imposing the feasibility constraint $\\int_{\\Omega}u=1$ as in (a.1), we get that\n$$\\exp(-1-\\mu) = \\frac{1}{\\int_{\\Omega}\\exp\\left(-\\langle\\lambda,g(x)\\rangle\\right){\\mathrm{d}}x},$$\nwhich upon sustutiting back to $u^{*}$, gives the desired result. As in (a.1), here again, we did not need to explicitly enforce $u\\geq 0$ since the exponential function returns nonnegative values.\n\n(b.2) Specializing the answer in part (b.1) for $\\Omega = [a,b]$, $m=1$, $g_1(x)=x$, $a_1 = \\overline{x}$, we get $u^{*}(x) = \\dfrac{e^{-\\lambda x}}{Z}$ where the constants $\\lambda,Z$ solve\n$$Z = \\displaystyle\\int_{a}^{b} e^{-\\lambda x}{\\rm{d}}x = \\dfrac{e^{-\\lambda a} - e^{-\\lambda b}}{\\lambda}, \\qquad Z\\:\\overline{x} = \\displaystyle\\int_{a}^{b} x e^{-\\lambda x}{\\rm{d}}x = \\dfrac{e^{-\\lambda a}\\left(\\lambda a + 1\\right) - e^{-\\lambda b}\\left(\\lambda b + 1\\right)}{\\lambda^2}.$$\nEquivalently, $\\lambda,Z$ solve the system\n$$\\lambda Z = e^{-\\lambda a} - e^{-\\lambda b}, \\qquad \\lambda Z\\:\\overline{x} + Z = ae^{-\\lambda a} - be^{-\\lambda b}.$$",
      "metadata": {}
    }
  ]
}